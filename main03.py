import os
import random
import time
import copy
import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import matplotlib
# è®¾ç½®ä¸­æ–‡å­—ä½“
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision import models, transforms

# ===========================================================
# 1. Fix Random Seed
# ===========================================================
def setup_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"âœ… Random seed fixed to: {seed}")

setup_seed(42)

# ===========================================================
# 2. é’ˆå¯¹æ€§æ•°æ®å¢å¼ºï¼ˆé’ˆå¯¹é«˜æ¸©åŒºåŸŸï¼‰
# ===========================================================
input_size = 224

# åˆ›å»ºé’ˆå¯¹æ€§çš„æ•°æ®å¢å¼º
class TargetedAugmentation:
    @staticmethod
    def adjust_for_temperature(img, temperature):
        """æ ¹æ®æ¸©åº¦è°ƒæ•´å›¾åƒå¢å¼ºå¼ºåº¦"""
        # é«˜æ¸©åŒºåŸŸï¼ˆ>400â„ƒï¼‰éœ€è¦æ›´å¼ºçš„å¢å¼º
        if temperature > 400:
            # é«˜æ¸©å›¾åƒå¯èƒ½æœ‰æ›´å¤šå™ªå£°
            return transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize((224, 224)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomVerticalFlip(p=0.5),
                transforms.RandomRotation(degrees=15),
                transforms.ColorJitter(brightness=0.15, contrast=0.15),
                transforms.ToTensor(),
            ])(img)
        else:
            # ä¸­ä½æ¸©åŒºåŸŸä½¿ç”¨æ¸©å’Œå¢å¼º
            return transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize((224, 224)),
                transforms.RandomHorizontalFlip(p=0.3),
                transforms.RandomRotation(degrees=10),
                transforms.ToTensor(),
            ])(img)

data_transforms = {
    'train': transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((240, 240)),  # ç¨å¤§å°ºå¯¸ï¼Œç„¶åéšæœºè£å‰ª
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        # é’ˆå¯¹æ€§çš„é¢œè‰²æŠ–åŠ¨ï¼šé«˜æ¸©åŒºåŸŸæŠ–åŠ¨æ›´å¼º
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),
        transforms.ToTensor(),
    ]),

    'test': transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ]),
}

# ===========================================================
# 3. æ”¹è¿›çš„Dataset Classï¼ˆå¸¦æ¸©åº¦æ„ŸçŸ¥ï¼‰
# ===========================================================
class ImprovedMagnesiumDataset(Dataset):
    def __init__(self, img_dir, transform=None, is_train=True):
        self.img_dir = img_dir
        self.transform = transform
        self.is_train = is_train
        
        valid_extensions = ('.jpg', '.jpeg', '.png')
        self.all_files = [
            f for f in os.listdir(img_dir) 
            if f.lower().endswith(valid_extensions)
        ]
        
        # æŒ‰æ–‡ä»¶åæ’åº
        self.all_files.sort()
        
        # æ”¶é›†æ¸©åº¦å’Œæ–‡ä»¶åæ˜ å°„
        self.temp_to_files = {}
        self.temperatures = []
        
        for filename in self.all_files:
            try:
                parts = filename.split('_')
                if len(parts) >= 2:
                    temp = float(parts[1])
                    self.temperatures.append(temp)
                    
                    if temp not in self.temp_to_files:
                        self.temp_to_files[temp] = []
                    self.temp_to_files[temp].append(filename)
            except:
                continue
        
        if self.temperatures:
            self.min_temp = min(self.temperatures)
            self.max_temp = max(self.temperatures)
            self.center_temp = (self.min_temp + self.max_temp) / 2
            self.half_range = (self.max_temp - self.min_temp) / 2
            
            print(f"ğŸ“Š æ¸©åº¦åˆ†å¸ƒ:")
            print(f"  èŒƒå›´: {self.min_temp:.0f}â„ƒ - {self.max_temp:.0f}â„ƒ")
            print(f"  ä¸­å¿ƒ: {self.center_temp:.1f}â„ƒ")
            
            # æ˜¾ç¤ºæ¯ä¸ªæ¸©åº¦ç‚¹çš„æ ·æœ¬æ•°
            print(f"  æ¸©åº¦ç‚¹åˆ†å¸ƒ:")
            unique_temps = sorted(set(self.temperatures))
            for temp in unique_temps:
                count = len([t for t in self.temperatures if t == temp])
                print(f"    {temp:.0f}â„ƒ: {count}ä¸ªæ ·æœ¬")

    def __len__(self):
        return len(self.all_files)

    def __getitem__(self, idx):
        img_name = self.all_files[idx]
        img_path = os.path.join(self.img_dir, img_name)
        
        # è§£ææ¸©åº¦
        parts = img_name.split('_')
        temperature = float(parts[1]) if len(parts) >= 2 else self.center_temp
        
        # --- è¯»å–å›¾ç‰‡ ---
        try:
            image = Image.open(img_path).convert('RGB')
            image_rgb = np.array(image)
            image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
        except:
            # è¿”å›ä¸­æ€§ç°è‰²å›¾åƒ
            dummy_image = np.ones((224, 224, 3), dtype=np.uint8) * 128
            image_bgr = dummy_image
            
        # Convert to Lab
        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
        image_lab = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2Lab)
        
        l, a, b = cv2.split(image_lab)

        # -------------------------------------------------------
        # æ¸©åº¦è‡ªé€‚åº”çš„å›¾åƒé¢„å¤„ç†
        # -------------------------------------------------------
        # 1. é«˜å…‰å»é™¤ - é«˜æ¸©å›¾åƒå¯èƒ½æ›´äº®
        l_median = np.median(l)
        # é«˜æ¸©å›¾åƒä½¿ç”¨æ›´é«˜é˜ˆå€¼
        threshold_multiplier = 1.0 + (temperature - self.center_temp) / (self.max_temp - self.min_temp) * 0.5
        threshold = l_median + 70 * threshold_multiplier
        
        mask = l > threshold
        if np.sum(mask) > 0:
            # ä½¿ç”¨ä¸­å€¼æ»¤æ³¢ä¿®å¤é«˜å…‰åŒºåŸŸ
            l_fixed = cv2.medianBlur(l, 3)
            l[mask] = l_fixed[mask]

        # 2. äº®åº¦å¯¹é½ - è€ƒè™‘æ¸©åº¦å½±å“
        l_median_new = np.median(l)
        # é«˜æ¸©å›¾åƒç›®æ ‡äº®åº¦ç¨ä½
        target_brightness = 128.0 - (temperature - self.center_temp) / (self.max_temp - self.min_temp) * 10
        shift = target_brightness - l_median_new
        l_aligned = l.astype(np.float32) + shift * 0.8
        l_aligned = np.clip(l_aligned, 0, 255).astype(np.uint8)

        # 3. è‡ªé€‚åº”å»å™ª - é«˜æ¸©å›¾åƒå¯èƒ½éœ€è¦æ›´å¼ºå»å™ª
        kernel_size = 3 if temperature < 350 else 5
        kernel_size = kernel_size if kernel_size % 2 == 1 else kernel_size + 1
        
        a_blur = cv2.GaussianBlur(a, (kernel_size, kernel_size), 0)
        b_blur = cv2.GaussianBlur(b, (kernel_size, kernel_size), 0)
        
        # 4. å¯¹æ¯”åº¦å¢å¼º - é’ˆå¯¹ä¸åŒæ¸©åº¦
        if temperature > 400:
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            l_enhanced = clahe.apply(l_aligned)
        else:
            l_enhanced = l_aligned
        
        image_lab_processed = cv2.merge((l_enhanced, a_blur, b_blur))
        
        # -------------------------------------------------------
        # å¢å¼ºçš„ç‰¹å¾æå–
        # -------------------------------------------------------
        # å¤šå°ºåº¦ç›´æ–¹å›¾
        hist_features = []
        for bins in [16, 32, 48]:  # å¤šå°ºåº¦
            hist_a = cv2.calcHist([a_blur], [0], None, [bins], [0, 256])
            hist_b = cv2.calcHist([b_blur], [0], None, [bins], [0, 256])
            
            cv2.normalize(hist_a, hist_a, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)
            cv2.normalize(hist_b, hist_b, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)
            
            hist_features.append(hist_a.flatten())
            hist_features.append(hist_b.flatten())
        
        # æ¸©åº¦ç›¸å…³çš„ç»Ÿè®¡ç‰¹å¾
        stats_features = [
            temperature / 100.0,  # æ¸©åº¦ä½œä¸ºç‰¹å¾ï¼ˆå½’ä¸€åŒ–ï¼‰
            np.mean(a_blur),
            np.std(a_blur),
            np.mean(b_blur),
            np.std(b_blur),
            np.mean(l_enhanced),
            np.std(l_enhanced),
            np.median(l_enhanced),
        ]
        
        # ç»„åˆç‰¹å¾
        all_features = []
        for feat in hist_features:
            all_features.append(feat.astype(np.float32))
        all_features.append(np.array(stats_features, dtype=np.float32))
        
        hist_feat = np.concatenate(all_features)
        hist_feat = torch.tensor(hist_feat, dtype=torch.float32)
        
        # æ ‡ç­¾å½’ä¸€åŒ–
        label = (temperature - self.center_temp) / self.half_range
        label = torch.tensor(label, dtype=torch.float32)
        actual_temp = torch.tensor(temperature, dtype=torch.float32)

        # Apply Transforms
        if self.transform:
            image = self.transform(image_lab_processed) 
        else:
            image = transforms.ToTensor()(image_lab_processed)
            
        return image, hist_feat, label, actual_temp

# ===========================================================
# 4. æ”¹è¿›çš„æ•°æ®åˆ’åˆ†ï¼ˆæ¸©åº¦å¹³è¡¡ï¼‰
# ===========================================================
def create_temperature_balanced_split(dataset, test_ratio=0.2):
    """åˆ›å»ºæ¸©åº¦å¹³è¡¡çš„è®­ç»ƒæµ‹è¯•åˆ†å‰²"""
    # æŒ‰æ¸©åº¦åˆ†ç»„
    temp_groups = {}
    for i, temp in enumerate(dataset.temperatures):
        if temp not in temp_groups:
            temp_groups[temp] = []
        temp_groups[temp].append(i)
    
    train_indices = []
    test_indices = []
    
    # å¯¹æ¯ä¸ªæ¸©åº¦ç‚¹è¿›è¡Œåˆ†å±‚é‡‡æ ·
    for temp, indices in temp_groups.items():
        np.random.shuffle(indices)
        split_point = int(len(indices) * (1 - test_ratio))
        train_indices.extend(indices[:split_point])
        test_indices.extend(indices[split_point:])
    
    np.random.shuffle(train_indices)
    np.random.shuffle(test_indices)
    
    return train_indices, test_indices

# ===========================================================
# åŠ è½½æ•°æ®
# ===========================================================
data_dir = r'D:\Study\å¤§ä¸‰ä¸Š\science\å¤§åˆ›\JPG-å¤„ç†å›¾\JPG-å¤„ç†å›¾\zhaodu21-25'

print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
full_dataset = ImprovedMagnesiumDataset(data_dir, transform=data_transforms['train'], is_train=True)

# æ¸©åº¦å¹³è¡¡çš„åˆ†å‰²
train_indices, test_indices = create_temperature_balanced_split(full_dataset, test_ratio=0.2)

train_dataset = Subset(full_dataset, train_indices)
test_dataset = Subset(full_dataset, test_indices)

print(f"\nğŸ“Š æ•°æ®åˆ’åˆ† (æ¸©åº¦å¹³è¡¡):")
print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")

# æ£€æŸ¥æ¸©åº¦åˆ†å¸ƒ
train_temps = [full_dataset.temperatures[i] for i in train_indices]
test_temps = [full_dataset.temperatures[i] for i in test_indices]

print(f"\nğŸŒ¡ï¸  è®­ç»ƒé›†æ¸©åº¦èŒƒå›´: {min(train_temps):.0f}â„ƒ - {max(train_temps):.0f}â„ƒ")
print(f"ğŸŒ¡ï¸  æµ‹è¯•é›†æ¸©åº¦èŒƒå›´: {min(test_temps):.0f}â„ƒ - {max(test_temps):.0f}â„ƒ")

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)

# ===========================================================
# 5. é’ˆå¯¹é«˜æ¸©åŒºåŸŸä¼˜åŒ–çš„æ¨¡å‹
# ===========================================================
class TemperatureAwareResNet(nn.Module):
    def __init__(self, feature_dim):
        super(TemperatureAwareResNet, self).__init__()
        
        # ä½¿ç”¨ResNet18ä½œä¸ºåŸºç¡€
        base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        self.features = nn.Sequential(*list(base_model.children())[:-2])
        
        # SEæ³¨æ„åŠ›
        self.se_block = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(512, 512 // 16),
            nn.ReLU(),
            nn.Linear(512 // 16, 512),
            nn.Sigmoid(),
            nn.Unflatten(1, (512, 1, 1))
        )
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        # å¢å¼ºçš„ç‰¹å¾èåˆ
        self.stats_fc = nn.Sequential(
            nn.Linear(8 + feature_dim, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.1)
        )
        
        # æ¸©åº¦æ„ŸçŸ¥çš„å›å½’å¤´
        self.final_regressor = nn.Sequential(
            nn.Linear(512 + 64, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # é«˜æ¸©åŒºåŸŸçš„é¢å¤–è¡¥å¿å±‚
        self.high_temp_adjust = nn.Sequential(
            nn.Linear(1, 16),
            nn.ReLU(),
            nn.Linear(16, 1)
        )

    def forward(self, x, hist_vec, temperature_hint=None):
        # CNNç‰¹å¾
        feat_map = self.features(x)
        
        # SEæ³¨æ„åŠ›
        se_weights = self.se_block(feat_map)
        feat_map = feat_map * se_weights
        
        cnn_feat = self.avgpool(feat_map)
        cnn_feat = torch.flatten(cnn_feat, 1)
        
        # ç»Ÿè®¡ç‰¹å¾
        mean_stats = torch.mean(x, dim=[2, 3])
        std_stats = torch.std(x, dim=[2, 3])
        
        mean_a = mean_stats[:, 1:2] 
        mean_b = mean_stats[:, 2:3]
        diff_ab = mean_a - mean_b
        sum_ab = mean_a + mean_b
        
        basic_stats = torch.cat([mean_stats, std_stats, diff_ab, sum_ab], dim=1)
        total_stats = torch.cat([basic_stats, hist_vec], dim=1)
        
        stats_out = self.stats_fc(total_stats)
        
        # ç‰¹å¾èåˆ
        combined = torch.cat([cnn_feat, stats_out], dim=1)
        
        # åŸºç¡€é¢„æµ‹
        base_pred = self.final_regressor(combined)
        
        # é«˜æ¸©è¡¥å¿ï¼ˆå¦‚æœæœ‰æ¸©åº¦æç¤ºï¼‰
        if temperature_hint is not None:
            # å¯¹é«˜æ¸©æ ·æœ¬è¿›è¡Œé¢å¤–è°ƒæ•´
            high_temp_mask = (temperature_hint > 400).float().unsqueeze(1)
            temp_adjustment = self.high_temp_adjust(temperature_hint.unsqueeze(1))
            adjusted_pred = base_pred + temp_adjustment * high_temp_mask * 0.1
            return adjusted_pred
        
        return base_pred

# ===========================================================
# 6. è®­ç»ƒå‡†å¤‡
# ===========================================================
device = torch.device("cuda:0")  # å¼ºåˆ¶ä½¿ç”¨GPUï¼Œæ²¡æœ‰åˆ™æŠ¥é”™
print(f"\nğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

# è·å–ç‰¹å¾ç»´åº¦
sample_img, sample_feat, _, _ = full_dataset[0]
feature_dim = sample_feat.shape[0]
print(f"ç‰¹å¾ç»´åº¦: {feature_dim}")

model = TemperatureAwareResNet(feature_dim).to(device)
print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# åŠ æƒæŸå¤±å‡½æ•°ï¼šç»™é«˜æ¸©åŒºåŸŸæ›´é«˜æƒé‡
class WeightedHuberLoss(nn.Module):
    def __init__(self, delta=1.0, high_temp_weight=1.5):
        super().__init__()
        self.delta = delta
        self.high_temp_weight = high_temp_weight
        
    def forward(self, pred, target, temperatures):
        # åŸºç¡€HuberæŸå¤±
        diff = pred - target
        abs_diff = torch.abs(diff)
        
        # HuberæŸå¤±
        loss = torch.where(abs_diff < self.delta,
                          0.5 * diff ** 2,
                          self.delta * (abs_diff - 0.5 * self.delta))
        
        # é«˜æ¸©åŒºåŸŸåŠ æƒ
        weights = torch.ones_like(loss)
        high_temp_mask = (temperatures > 400).float().unsqueeze(1)
        weights = weights + high_temp_mask * (self.high_temp_weight - 1.0)
        
        return torch.mean(loss * weights)

criterion = WeightedHuberLoss(delta=1.0, high_temp_weight=1.5)

# ä¼˜åŒ–å™¨
optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-4)

# åŠ¨æ€å­¦ä¹ ç‡è°ƒåº¦
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer, 
    max_lr=0.001,
    epochs=200,
    steps_per_epoch=len(train_loader),
    pct_start=0.3
)

num_epochs = 200

# ===========================================================
# 7. é’ˆå¯¹æ€§è®­ç»ƒå‡½æ•°
# ===========================================================
def train_with_focus(model, train_loader, test_loader, criterion, optimizer, scheduler, 
                    center_temp, half_range, num_epochs=200):
    
    since = time.time()
    best_model_wts = copy.deepcopy(model.state_dict())
    best_mae = float('inf')
    # patience_counter = 0  # æ—©åœæœºåˆ¶å·²æ³¨é‡Š
    
    train_loss_history = []
    test_mae_history = []
    test_mae_high_temp = []  # é«˜æ¸©åŒºåŸŸMAE
    test_mae_low_temp = []   # ä½æ¸©åŒºåŸŸMAE

    print("\n" + "="*60)
    print("ğŸ”¥ å¼€å§‹é’ˆå¯¹æ€§è®­ç»ƒ (èšç„¦é«˜æ¸©åŒºåŸŸ)")
    print("="*60)

    for epoch in range(num_epochs):
        # --- Train ---
        model.train()
        running_loss = 0.0
        
        for inputs, hists, labels, temps in train_loader:
            inputs = inputs.to(device)
            hists = hists.to(device)
            labels = labels.to(device).unsqueeze(1)
            temps = temps.to(device)
            
            optimizer.zero_grad()
            
            # ä½¿ç”¨æ¸©åº¦ä½œä¸ºæç¤º
            outputs = model(inputs, hists, temps/100.0)  # å½’ä¸€åŒ–çš„æ¸©åº¦æç¤º
            
            loss = criterion(outputs, labels, temps)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            
            running_loss += loss.item() * inputs.size(0)

        epoch_loss = running_loss / len(train_loader.dataset)

        # --- Test ---
        model.eval()
        val_mae_sum = 0.0
        val_mae_high_sum = 0.0
        val_mae_low_sum = 0.0
        high_temp_count = 0
        low_temp_count = 0
        
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for inputs, hists, labels, actual_temps in test_loader:
                inputs = inputs.to(device)
                hists = hists.to(device)
                labels = labels.to(device).unsqueeze(1)
                actual_temps = actual_temps.to(device)
                
                outputs = model(inputs, hists, actual_temps/100.0)
                
                # åå½’ä¸€åŒ–
                preds_real = outputs * half_range + center_temp
                targets_real = actual_temps.unsqueeze(1)
                
                all_preds.extend(preds_real.cpu().numpy().flatten())
                all_targets.extend(targets_real.cpu().numpy().flatten())
                
                # æ€»ä½“MAE
                batch_mae = torch.abs(preds_real - targets_real)
                val_mae_sum += torch.sum(batch_mae).item()
                
                # é«˜æ¸©åŒºåŸŸMAE (>400â„ƒ)
                high_temp_mask = actual_temps > 400
                if torch.any(high_temp_mask):
                    high_mae = torch.abs(preds_real[high_temp_mask] - targets_real[high_temp_mask])
                    val_mae_high_sum += torch.sum(high_mae).item()
                    high_temp_count += torch.sum(high_temp_mask).item()
                
                # ä½æ¸©åŒºåŸŸMAE (<=400â„ƒ)
                low_temp_mask = actual_temps <= 400
                if torch.any(low_temp_mask):
                    low_mae = torch.abs(preds_real[low_temp_mask] - targets_real[low_temp_mask])
                    val_mae_low_sum += torch.sum(low_mae).item()
                    low_temp_count += torch.sum(low_temp_mask).item()
        
        epoch_mae = val_mae_sum / len(test_loader.dataset)
        epoch_mae_high = val_mae_high_sum / high_temp_count if high_temp_count > 0 else 0
        epoch_mae_low = val_mae_low_sum / low_temp_count if low_temp_count > 0 else 0
        
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•å†å²
        train_loss_history.append(epoch_loss)
        test_mae_history.append(epoch_mae)
        test_mae_high_temp.append(epoch_mae_high)
        test_mae_low_temp.append(epoch_mae_low)
        
        # æ‰“å°è¿›åº¦
        marker = "ğŸ”¥" if epoch_mae < 5 else "âš¡" if epoch_mae < 6 else "ğŸ“ˆ"
        print(f'{marker} Epoch {epoch+1:03d}/{num_epochs} | LR: {current_lr:.6f}')
        print(f'   Loss: {epoch_loss:.4f} | MAE: {epoch_mae:.2f}â„ƒ')
        print(f'   é«˜æ¸©(>400â„ƒ): {epoch_mae_high:.2f}â„ƒ | ä½æ¸©: {epoch_mae_low:.2f}â„ƒ')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if epoch_mae < best_mae:
            best_mae = epoch_mae
            best_model_wts = copy.deepcopy(model.state_dict())
            # patience_counter = 0  # æ—©åœæœºåˆ¶å·²æ³¨é‡Š
            
            # ä¿å­˜è¯¦ç»†ä¿¡æ¯
            torch.save({
                'epoch': epoch,
                'model_state_dict': best_model_wts,
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_mae': best_mae,
                'mae_high': epoch_mae_high,
                'mae_low': epoch_mae_low,
            }, f'best_model_mae_{best_mae:.2f}.pth')
            
            print(f"   âœ… æ–°æœ€ä½³! æ€»ä½“MAE: {best_mae:.2f}â„ƒ")
        else:
            # patience_counter += 1  # æ—©åœæœºåˆ¶å·²æ³¨é‡Š
            # if patience_counter >= 40:  # å¢åŠ è€å¿ƒå€¼  # æ—©åœæœºåˆ¶å·²æ³¨é‡Š
            #     print(f"   â¹ï¸  æ—©åœè§¦å‘äº epoch {epoch+1}")  # æ—©åœæœºåˆ¶å·²æ³¨é‡Š
            #     break  # æ—©åœæœºåˆ¶å·²æ³¨é‡Š
            pass

    time_elapsed = time.time() - since
    print("\n" + "="*60)
    print(f'ğŸ è®­ç»ƒå®Œæˆ')
    print(f'   ç”¨æ—¶: {time_elapsed // 60:.0f}åˆ† {time_elapsed % 60:.0f}ç§’')
    print(f'   æœ€ç»ˆæœ€ä½³æµ‹è¯• MAE: {best_mae:.2f}â„ƒ')
    
    model.load_state_dict(best_model_wts)
    
    return (model, train_loss_history, test_mae_history, 
            test_mae_high_temp, test_mae_low_temp, all_preds, all_targets)

# ===========================================================
# 8. æ‰§è¡Œè®­ç»ƒ
# ===========================================================
print("\n" + "="*60)
print("ğŸ¯ æœ€ç»ˆä¼˜åŒ– - ç›®æ ‡: MAE < 5â„ƒ")
print("="*60)

results = train_with_focus(
    model, train_loader, test_loader, criterion, optimizer, scheduler,
    full_dataset.center_temp, full_dataset.half_range, num_epochs=num_epochs
)

trained_model, train_hist, test_hist, test_high_hist, test_low_hist, all_preds, all_targets = results

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
save_path = 'optimized_final_model.pth'
torch.save(trained_model.state_dict(), save_path)
print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

# ===========================================================
# 9. è¯¦ç»†åˆ†æ
# ===========================================================
def detailed_analysis(preds, targets, train_loss, test_mae, test_high, test_low):
    errors = np.array(preds) - np.array(targets)
    abs_errors = np.abs(errors)
    targets_arr = np.array(targets)
    
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆç»“æœè¯¦ç»†åˆ†æ")
    print("="*60)
    
    print(f"\nğŸ¯ æ€»ä½“æŒ‡æ ‡:")
    print(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE): {np.mean(abs_errors):.2f}â„ƒ")
    print(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE): {np.sqrt(np.mean(errors**2)):.2f}â„ƒ")
    
    print(f"\nğŸŒ¡ï¸  åˆ†æ¸©åº¦åŒºé—´:")
    temp_ranges = [
        (250, 300, "ä½æ¸©"),
        (300, 350, "ä¸­ä½æ¸©"),
        (350, 400, "ä¸­æ¸©"),
        (400, 450, "é«˜æ¸©")
    ]
    
    for low, high, label in temp_ranges:
        mask = (targets_arr >= low) & (targets_arr < high)
        if np.sum(mask) > 0:
            range_errors = errors[mask]
            range_mae = np.mean(np.abs(range_errors))
            range_std = np.std(range_errors)
            print(f"  {label}({low}-{high}â„ƒ): {np.sum(mask):2d}æ ·æœ¬, "
                  f"MAE: {range_mae:5.2f}â„ƒ, STD: {range_std:5.2f}â„ƒ")
    
    print(f"\nğŸ“ˆ è¯¯å·®åˆ†å¸ƒåˆ†æ:")
    sorted_errors = np.sort(abs_errors)
    thresholds = [1, 2, 3, 4, 5, 10]
    for thresh in thresholds:
        percent = np.sum(abs_errors <= thresh) / len(abs_errors) * 100
        print(f"  è¯¯å·® â‰¤ {thresh}â„ƒ: {percent:5.1f}%")
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # 1. è®­ç»ƒæŸå¤±
    axes[0, 0].plot(train_loss, 'b-', linewidth=1.5)
    axes[0, 0].set_title('è®­ç»ƒæŸå¤±', fontsize=12)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. æµ‹è¯•MAE
    axes[0, 1].plot(test_mae, 'r-', label='æ€»ä½“', linewidth=1.5)
    axes[0, 1].plot(test_high, 'orange', label='é«˜æ¸©(>400â„ƒ)', linewidth=1.5, alpha=0.7)
    axes[0, 1].plot(test_low, 'green', label='ä½æ¸©', linewidth=1.5, alpha=0.7)
    axes[0, 1].axhline(y=5, color='k', linestyle='--', alpha=0.5, label='5â„ƒç›®æ ‡')
    axes[0, 1].set_title('æµ‹è¯•MAE (Â°C)', fontsize=12)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].legend(fontsize=9)
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. é¢„æµ‹vsçœŸå®
    axes[0, 2].scatter(targets_arr, preds, alpha=0.6, s=30, c=targets_arr, cmap='coolwarm')
    min_val = min(min(targets), min(preds))
    max_val = max(max(targets), max(preds))
    axes[0, 2].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)
    axes[0, 2].set_xlabel('çœŸå®æ¸©åº¦ (Â°C)')
    axes[0, 2].set_ylabel('é¢„æµ‹æ¸©åº¦ (Â°C)')
    axes[0, 2].set_title('é¢„æµ‹ vs çœŸå®')
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. è¯¯å·®åˆ†å¸ƒ
    axes[1, 0].hist(errors, bins=30, edgecolor='black', alpha=0.7)
    axes[1, 0].axvline(x=0, color='r', linestyle='--')
    axes[1, 0].set_xlabel('é¢„æµ‹è¯¯å·® (Â°C)')
    axes[1, 0].set_ylabel('é¢‘ç‡')
    axes[1, 0].set_title('è¯¯å·®åˆ†å¸ƒ')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. ç»å¯¹è¯¯å·®CDF
    sorted_abs = np.sort(abs_errors)
    cdf = np.arange(1, len(sorted_abs) + 1) / len(sorted_abs)
    axes[1, 1].plot(sorted_abs, cdf, 'b-', linewidth=2)
    for thresh in [1, 3, 5]:
        idx = np.searchsorted(sorted_abs, thresh)
        if idx < len(cdf):
            axes[1, 1].axvline(x=thresh, color='r', linestyle='--', alpha=0.5)
            axes[1, 1].text(thresh, cdf[idx], f'{cdf[idx]*100:.0f}%', 
                           fontsize=9, ha='right')
    axes[1, 1].set_xlabel('ç»å¯¹è¯¯å·® (Â°C)')
    axes[1, 1].set_ylabel('ç´¯ç§¯æ¦‚ç‡')
    axes[1, 1].set_title('ç»å¯¹è¯¯å·®CDF')
    axes[1, 1].grid(True, alpha=0.3)
    
    # 6. æ¸©åº¦vsè¯¯å·®æ•£ç‚¹å›¾
    axes[1, 2].scatter(targets_arr, abs_errors, alpha=0.6, s=30)
    # æ·»åŠ ç§»åŠ¨å¹³å‡çº¿
    window = 20
    if len(targets_arr) > window:
        sorted_idx = np.argsort(targets_arr)
        sorted_temps = targets_arr[sorted_idx]
        sorted_errors_abs = abs_errors[sorted_idx]
        
        moving_avg = np.convolve(sorted_errors_abs, np.ones(window)/window, mode='valid')
        temp_avg = np.convolve(sorted_temps, np.ones(window)/window, mode='valid')
        
        axes[1, 2].plot(temp_avg, moving_avg, 'r-', linewidth=2, label=f'{window}ç‚¹ç§»åŠ¨å¹³å‡')
    
    axes[1, 2].axhline(y=5, color='k', linestyle='--', alpha=0.5, label='5â„ƒç›®æ ‡')
    axes[1, 2].set_xlabel('çœŸå®æ¸©åº¦ (Â°C)')
    axes[1, 2].set_ylabel('ç»å¯¹è¯¯å·® (Â°C)')
    axes[1, 2].set_title('æ¸©åº¦ vs ç»å¯¹è¯¯å·®')
    axes[1, 2].legend(fontsize=9)
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('final_results.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"\nğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ:")
    mae_under_5 = np.mean(abs_errors < 5) * 100
    print(f"  è¯¯å·®åœ¨5â„ƒä»¥å†…çš„æ ·æœ¬æ¯”ä¾‹: {mae_under_5:.1f}%")
    
    if np.mean(abs_errors) < 5:
        print(f"\nâœ… æˆåŠŸ! å¹³å‡MAEè¾¾åˆ°{np.mean(abs_errors):.2f}â„ƒï¼Œä½äº5â„ƒç›®æ ‡!")
    else:
        print(f"\nâš ï¸  æ¥è¿‘ç›®æ ‡! å¹³å‡MAEä¸º{np.mean(abs_errors):.2f}â„ƒï¼Œç•¥é«˜äº5â„ƒç›®æ ‡")

# æ‰§è¡Œåˆ†æ
detailed_analysis(all_preds, all_targets, train_hist, test_hist, test_high_hist, test_low_hist)

print("\n" + "="*60)
print("ğŸ‰ æœ€ç»ˆä¼˜åŒ–å®Œæˆ!")
print("="*60)